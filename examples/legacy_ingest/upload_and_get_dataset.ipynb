{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Database Ingestion and Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "For this example, I will be creating a local modeling database that will live at `/database/local_store.db`. If you look at the `bash.sh` and `jupyter.sh` scripts for this project you will see how I am mounting those drives so that I can reuse this local database both on my base OS and in the basic docker image for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "# run these uploads locally\n",
    "serv = \"local\"\n",
    "configs = pathlib.Path(\"/database/configs.json\")\n",
    "with open(configs) as read_in:\n",
    "    configs = json.load(read_in)\n",
    "\n",
    "# setting local store path\n",
    "configs[serv][serv][\"database\"] = str(pathlib.Path(\"/database/local_store.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from orator import DatabaseManager\n",
    "from modelingdbtools import query\n",
    "from modelingdbtools import ingest\n",
    "from modelingdbtools.utils import admin\n",
    "from modelingdbtools.schemas import modeling\n",
    "\n",
    "# create database connection\n",
    "db = DatabaseManager(configs[serv])\n",
    "# create and fill tables with basic data\n",
    "modeling.create_schema(db)\n",
    "modeling.add_schema_data(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Dataset\n",
    "\n",
    "The next cell will create an ingestion test dataset and a csv copy of the dataset saved to `/active/fp_example/example.csv` to show the capabilities of how data can be pushed to each instance of a modeling database using `modelingdbtools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bools</th>\n",
       "      <th>files</th>\n",
       "      <th>floats</th>\n",
       "      <th>ndarrays</th>\n",
       "      <th>sets</th>\n",
       "      <th>strings</th>\n",
       "      <th>tuples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/0.json</td>\n",
       "      <td>66.898752</td>\n",
       "      <td>[[0.9484552616759917, 0.09563095988658465], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo0</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/1.json</td>\n",
       "      <td>93.264795</td>\n",
       "      <td>[[0.28620255259521155, 0.4880146924030817], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo1</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/2.json</td>\n",
       "      <td>47.566390</td>\n",
       "      <td>[[0.36726405042616705, 0.15877060302386292], [...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo2</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/3.json</td>\n",
       "      <td>39.528498</td>\n",
       "      <td>[[0.3458709444410156, 0.3891204565683706], [0....</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo3</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/4.json</td>\n",
       "      <td>63.992229</td>\n",
       "      <td>[[0.45668750745607944, 0.7918647424935192], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo4</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/5.json</td>\n",
       "      <td>51.968740</td>\n",
       "      <td>[[0.8840716964271916, 0.5458374928833444], [0....</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo5</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/6.json</td>\n",
       "      <td>79.640487</td>\n",
       "      <td>[[0.4821382601924946, 0.19950456722334786], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo6</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/7.json</td>\n",
       "      <td>42.239606</td>\n",
       "      <td>[[0.6015425552752097, 0.48554392851913886], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo7</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/8.json</td>\n",
       "      <td>43.779890</td>\n",
       "      <td>[[0.5137284582530655, 0.7450085355787271], [0....</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo8</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/9.json</td>\n",
       "      <td>74.309742</td>\n",
       "      <td>[[0.15560902770220908, 0.6947611668364451], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo9</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bools                      files     floats  \\\n",
       "0   True  /active/fp_example/0.json  66.898752   \n",
       "1  False  /active/fp_example/1.json  93.264795   \n",
       "2  False  /active/fp_example/2.json  47.566390   \n",
       "3  False  /active/fp_example/3.json  39.528498   \n",
       "4   True  /active/fp_example/4.json  63.992229   \n",
       "5  False  /active/fp_example/5.json  51.968740   \n",
       "6  False  /active/fp_example/6.json  79.640487   \n",
       "7   True  /active/fp_example/7.json  42.239606   \n",
       "8  False  /active/fp_example/8.json  43.779890   \n",
       "9  False  /active/fp_example/9.json  74.309742   \n",
       "\n",
       "                                            ndarrays       sets strings  \\\n",
       "0  [[0.9484552616759917, 0.09563095988658465], [0...  {1, 2, 3}    foo0   \n",
       "1  [[0.28620255259521155, 0.4880146924030817], [0...  {1, 2, 3}    foo1   \n",
       "2  [[0.36726405042616705, 0.15877060302386292], [...  {1, 2, 3}    foo2   \n",
       "3  [[0.3458709444410156, 0.3891204565683706], [0....  {1, 2, 3}    foo3   \n",
       "4  [[0.45668750745607944, 0.7918647424935192], [0...  {1, 2, 3}    foo4   \n",
       "5  [[0.8840716964271916, 0.5458374928833444], [0....  {1, 2, 3}    foo5   \n",
       "6  [[0.4821382601924946, 0.19950456722334786], [0...  {1, 2, 3}    foo6   \n",
       "7  [[0.6015425552752097, 0.48554392851913886], [0...  {1, 2, 3}    foo7   \n",
       "8  [[0.5137284582530655, 0.7450085355787271], [0....  {1, 2, 3}    foo8   \n",
       "9  [[0.15560902770220908, 0.6947611668364451], [0...  {1, 2, 3}    foo9   \n",
       "\n",
       "      tuples  \n",
       "0  (1, 2, 3)  \n",
       "1  (1, 2, 3)  \n",
       "2  (1, 2, 3)  \n",
       "3  (1, 2, 3)  \n",
       "4  (1, 2, 3)  \n",
       "5  (1, 2, 3)  \n",
       "6  (1, 2, 3)  \n",
       "7  (1, 2, 3)  \n",
       "8  (1, 2, 3)  \n",
       "9  (1, 2, 3)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create test upload dataset\n",
    "fp_ex = pathlib.Path(\"/active/fp_example/\")\n",
    "if not os.path.exists(fp_ex):\n",
    "    os.makedirs(fp_ex)\n",
    "\n",
    "test = []\n",
    "for i in range(10):\n",
    "    fp =  fp_ex / (str(i) + \".json\")\n",
    "    with open(fp, \"w\") as write_out:\n",
    "        json.dump({\"hello\": \"world\"}, write_out)\n",
    "    \n",
    "    d = {}\n",
    "    d[\"strings\"] = \"foo\" + str(i)\n",
    "    d[\"bools\"] = np.random.rand() < 0.5\n",
    "    d[\"floats\"] = np.random.rand() * 100\n",
    "    d[\"ndarrays\"] = np.random.rand(2, 2)\n",
    "    d[\"tuples\"] = tuple([1, 2, 3])\n",
    "    d[\"sets\"] = set([1, 2, 3, 3, 3])\n",
    "    d[\"files\"] = str(fp)\n",
    "    test.append(d)\n",
    "\n",
    "test = pd.DataFrame(test)\n",
    "test.to_csv(fp_ex / \"example.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the Dataset\n",
    "\n",
    "What we just did was make a dataset with various types of values to show that the database will handle all classes of data that can be stringified. There are various reasons for doing this, mainly execution time to query, merge, and join datasets when we only want certain columns from each row being the largest. This could have been done in NoSQL but would have required much more time to develop and as a starting proof-of-concept this worked well to get it into peoples hands and testing the capabilities. Development on a NoSQL version may come later but for now SQL is suiting us fine.\n",
    "\n",
    "With various types created however let's upload and see how the database handles the data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dataset with that name already exists. Adding new version.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DatasetId</th>\n",
       "      <th>Description</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>this is the hello world of dataset ingestion</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.129518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DatasetId                                   Description  \\\n",
       "0        100  this is the hello world of dataset ingestion   \n",
       "\n",
       "                                       Name  \n",
       "0  test_dataset@@2018-06-16 03:33:26.129518  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload the test dataset to the connected db\n",
    "# map each column to a python class/ object\n",
    "# indicate not to upload the files to the fms\n",
    "# indicate which files should be checked for existance\n",
    "ds_info = ingest.upload_dataset(database=db,\n",
    "                                dataset=test,\n",
    "                                name=\"test_dataset\",\n",
    "                                description=\"this is the hello world of dataset ingestion\",\n",
    "                                type_map={\"bools\": bool, \n",
    "                                          \"files\": str, \n",
    "                                          \"floats\": float, \n",
    "                                          \"ndarrays\": np.ndarray, \n",
    "                                          \"strings\": str},\n",
    "                                upload_files=False,\n",
    "                                filepath_columns=[\"files\"])\n",
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any of this information is stored, the entire dataset is validated for two things, does all the data match the typing provided by the `type_map` parameters and do all the files exist in the columns provided by the `filepath_columns` parameter. Once the dataset is validated, what is actually happening on the backend is that the dataframe is being broken into individual key value pairings each with a source, sourcetype, group (row), valuetype, and creation datetime. This allows you to reduce down datasets to only keys (columns) you care about incredibly quickly, and return back to the original data typing when ingestion occured.\n",
    "\n",
    "If you want to validate a dataframe but do not want to upload the dataset to the database you can use:\n",
    "\n",
    "```\n",
    "from modelingdbtools.utils import checks\n",
    "checks.validate_dataset(dataset, type_map, filepath_columns)\n",
    "```\n",
    "\n",
    "I built a casting function that casts the string values stored back to their original types (in my opinion it covers pretty much all the types that you should really stick into the database, anything else and I think you may be trying to shove too much into the database... ([details on value casting here](./testing_decode_types.ipynb))\n",
    "\n",
    "To get the dataset pass either `ds_info[\"DatasetId\"]` or `ds_info[\"Name\"]` into `modelingdbtools.query.get_dataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bools</th>\n",
       "      <th>files</th>\n",
       "      <th>floats</th>\n",
       "      <th>ndarrays</th>\n",
       "      <th>sets</th>\n",
       "      <th>strings</th>\n",
       "      <th>tuples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/0.json</td>\n",
       "      <td>66.898752</td>\n",
       "      <td>[[0.9484552616759917, 0.09563095988658465], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo0</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/1.json</td>\n",
       "      <td>93.264795</td>\n",
       "      <td>[[0.28620255259521155, 0.4880146924030817], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo1</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/2.json</td>\n",
       "      <td>47.566390</td>\n",
       "      <td>[[0.36726405042616705, 0.15877060302386292], [...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo2</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/3.json</td>\n",
       "      <td>39.528498</td>\n",
       "      <td>[[0.3458709444410156, 0.3891204565683706], [0....</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo3</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/4.json</td>\n",
       "      <td>63.992229</td>\n",
       "      <td>[[0.45668750745607944, 0.7918647424935192], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo4</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/5.json</td>\n",
       "      <td>51.968740</td>\n",
       "      <td>[[0.8840716964271916, 0.5458374928833444], [0....</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo5</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/6.json</td>\n",
       "      <td>79.640487</td>\n",
       "      <td>[[0.4821382601924946, 0.19950456722334786], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo6</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/7.json</td>\n",
       "      <td>42.239606</td>\n",
       "      <td>[[0.6015425552752097, 0.48554392851913886], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo7</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/8.json</td>\n",
       "      <td>43.779890</td>\n",
       "      <td>[[0.5137284582530655, 0.7450085355787271], [0....</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo8</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/9.json</td>\n",
       "      <td>74.309742</td>\n",
       "      <td>[[0.15560902770220908, 0.6947611668364451], [0...</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo9</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bools                      files     floats  \\\n",
       "0   True  /active/fp_example/0.json  66.898752   \n",
       "1  False  /active/fp_example/1.json  93.264795   \n",
       "2  False  /active/fp_example/2.json  47.566390   \n",
       "3  False  /active/fp_example/3.json  39.528498   \n",
       "4   True  /active/fp_example/4.json  63.992229   \n",
       "5  False  /active/fp_example/5.json  51.968740   \n",
       "6  False  /active/fp_example/6.json  79.640487   \n",
       "7   True  /active/fp_example/7.json  42.239606   \n",
       "8  False  /active/fp_example/8.json  43.779890   \n",
       "9  False  /active/fp_example/9.json  74.309742   \n",
       "\n",
       "                                            ndarrays       sets strings  \\\n",
       "0  [[0.9484552616759917, 0.09563095988658465], [0...  {1, 2, 3}    foo0   \n",
       "1  [[0.28620255259521155, 0.4880146924030817], [0...  {1, 2, 3}    foo1   \n",
       "2  [[0.36726405042616705, 0.15877060302386292], [...  {1, 2, 3}    foo2   \n",
       "3  [[0.3458709444410156, 0.3891204565683706], [0....  {1, 2, 3}    foo3   \n",
       "4  [[0.45668750745607944, 0.7918647424935192], [0...  {1, 2, 3}    foo4   \n",
       "5  [[0.8840716964271916, 0.5458374928833444], [0....  {1, 2, 3}    foo5   \n",
       "6  [[0.4821382601924946, 0.19950456722334786], [0...  {1, 2, 3}    foo6   \n",
       "7  [[0.6015425552752097, 0.48554392851913886], [0...  {1, 2, 3}    foo7   \n",
       "8  [[0.5137284582530655, 0.7450085355787271], [0....  {1, 2, 3}    foo8   \n",
       "9  [[0.15560902770220908, 0.6947611668364451], [0...  {1, 2, 3}    foo9   \n",
       "\n",
       "      tuples  \n",
       "0  (1, 2, 3)  \n",
       "1  (1, 2, 3)  \n",
       "2  (1, 2, 3)  \n",
       "3  (1, 2, 3)  \n",
       "4  (1, 2, 3)  \n",
       "5  (1, 2, 3)  \n",
       "6  (1, 2, 3)  \n",
       "7  (1, 2, 3)  \n",
       "8  (1, 2, 3)  \n",
       "9  (1, 2, 3)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the dataset we just uploaded\n",
    "query.get_dataset(db, id=ds_info[\"DatasetId\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This same ingestion process can handle a csv filepath as well however there are some limitations in that when `pandasDataFrame`s are sent to a csv they lose complex typing such as `numpy.ndarray`. In this case we will specify that the `ndarrays` column should be validated as `string`.\n",
    "\n",
    "Validation of `string` can be done by explictly stating so, or by leaving the that key in the `type_map` blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dataset with that name already exists. Adding new version.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DatasetId</th>\n",
       "      <th>Description</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>this is the hello world of dataset ingestion</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.858881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DatasetId                                   Description  \\\n",
       "0        101  this is the hello world of dataset ingestion   \n",
       "\n",
       "                                       Name  \n",
       "0  test_dataset@@2018-06-16 03:33:26.858881  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also upload this with a csv filepath\n",
    "# you may lose typing on complex types though (ndarrays -> str)\n",
    "ds_info = ingest.upload_dataset(database=db,\n",
    "                                name=\"test_dataset\",\n",
    "                                dataset=(fp_ex / \"example.csv\"),\n",
    "                                description=\"this is the hello world of dataset ingestion\",\n",
    "                                type_map={\"bools\": bool, \n",
    "                                          \"files\": str, \n",
    "                                          \"floats\": float, \n",
    "                                          \"strings\": str},\n",
    "                                upload_files=False,\n",
    "                                filepath_columns=[\"files\"])\n",
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try filtering down the dataset we just uploaded to only return the `ndarray`s but before we do so, note that because we said to use the same dataset name as before, this upload was given a version (datetime) attached to the name. Additionally, if you want to get all the info regarding each `Iota` then use the `get_info_items` parameter.\n",
    "\n",
    "Note: `columns` parameter can be passed a `string` or `list of strings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndarrays</th>\n",
       "      <th>ndarrays(IotaId)</th>\n",
       "      <th>ndarrays(SourceId)</th>\n",
       "      <th>ndarrays(SourceTypeId)</th>\n",
       "      <th>ndarrays(Type)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.94845526 0.09563096] [0.13997903 0.69083119]]</td>\n",
       "      <td>6985</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.858881</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.28620255 0.48801469] [0.75724778 0.62419953]]</td>\n",
       "      <td>6993</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.858881</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.36726405 0.1587706 ] [0.97737339 0.48907071]]</td>\n",
       "      <td>7001</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.858881</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.34587094 0.38912046] [0.4764796  0.31096103]]</td>\n",
       "      <td>7009</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.858881</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.45668751 0.79186474] [0.77771222 0.53197103]]</td>\n",
       "      <td>7017</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.858881</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[0.8840717  0.54583749] [0.53723378 0.15321238]]</td>\n",
       "      <td>7025</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.858881</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[0.48213826 0.19950457] [0.71628275 0.33511008]]</td>\n",
       "      <td>7033</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.858881</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[0.60154256 0.48554393] [0.0889528  0.18535356]]</td>\n",
       "      <td>7041</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.858881</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[0.51372846 0.74500854] [0.80058246 0.10870908]]</td>\n",
       "      <td>7049</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.858881</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[0.15560903 0.69476117] [0.98950023 0.50251443]]</td>\n",
       "      <td>7057</td>\n",
       "      <td>test_dataset@@2018-06-16 03:33:26.858881</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ndarrays  ndarrays(IotaId)  \\\n",
       "0  [[0.94845526 0.09563096] [0.13997903 0.69083119]]              6985   \n",
       "1  [[0.28620255 0.48801469] [0.75724778 0.62419953]]              6993   \n",
       "2  [[0.36726405 0.1587706 ] [0.97737339 0.48907071]]              7001   \n",
       "3  [[0.34587094 0.38912046] [0.4764796  0.31096103]]              7009   \n",
       "4  [[0.45668751 0.79186474] [0.77771222 0.53197103]]              7017   \n",
       "5  [[0.8840717  0.54583749] [0.53723378 0.15321238]]              7025   \n",
       "6  [[0.48213826 0.19950457] [0.71628275 0.33511008]]              7033   \n",
       "7  [[0.60154256 0.48554393] [0.0889528  0.18535356]]              7041   \n",
       "8  [[0.51372846 0.74500854] [0.80058246 0.10870908]]              7049   \n",
       "9  [[0.15560903 0.69476117] [0.98950023 0.50251443]]              7057   \n",
       "\n",
       "                         ndarrays(SourceId)  ndarrays(SourceTypeId)  \\\n",
       "0  test_dataset@@2018-06-16 03:33:26.858881                       1   \n",
       "1  test_dataset@@2018-06-16 03:33:26.858881                       1   \n",
       "2  test_dataset@@2018-06-16 03:33:26.858881                       1   \n",
       "3  test_dataset@@2018-06-16 03:33:26.858881                       1   \n",
       "4  test_dataset@@2018-06-16 03:33:26.858881                       1   \n",
       "5  test_dataset@@2018-06-16 03:33:26.858881                       1   \n",
       "6  test_dataset@@2018-06-16 03:33:26.858881                       1   \n",
       "7  test_dataset@@2018-06-16 03:33:26.858881                       1   \n",
       "8  test_dataset@@2018-06-16 03:33:26.858881                       1   \n",
       "9  test_dataset@@2018-06-16 03:33:26.858881                       1   \n",
       "\n",
       "  ndarrays(Type)  \n",
       "0  <class 'str'>  \n",
       "1  <class 'str'>  \n",
       "2  <class 'str'>  \n",
       "3  <class 'str'>  \n",
       "4  <class 'str'>  \n",
       "5  <class 'str'>  \n",
       "6  <class 'str'>  \n",
       "7  <class 'str'>  \n",
       "8  <class 'str'>  \n",
       "9  <class 'str'>  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get ndarrays columns from the dataset we just uploaded\n",
    "query.get_dataset(db, id=ds_info[\"DatasetId\"][0], columns=\"ndarrays\", get_info_items=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Upload Requirements\n",
    "\n",
    "Below is an example of the minimum parameters needed to successfully upload a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DatasetId</th>\n",
       "      <th>Description</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102</td>\n",
       "      <td>None</td>\n",
       "      <td>jacksonb@@2018-06-16 03:33:27.575764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DatasetId Description                                  Name\n",
       "0        102        None  jacksonb@@2018-06-16 03:33:27.575764"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minimum you need to pass to upload a dataset\n",
    "ds_info = ingest.upload_dataset(database=db, dataset=test)\n",
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset name genereated was the `{user}@@{upload_datetime}`. If you are doing a minimal upload with a filepath instead of a `pandas.DataFrame` the name is generated using `{absolute_filepath}@@{upload_datetime}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset we just uploaded\n",
    "out = query.get_dataset(db, id=ds_info[\"DatasetId\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bools</th>\n",
       "      <th>files</th>\n",
       "      <th>floats</th>\n",
       "      <th>ndarrays</th>\n",
       "      <th>sets</th>\n",
       "      <th>strings</th>\n",
       "      <th>tuples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            bools          files           floats                 ndarrays  \\\n",
       "0  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "1  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "2  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "3  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "4  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "5  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "6  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "7  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "8  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "9  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "\n",
       "            sets        strings           tuples  \n",
       "0  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "1  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "2  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "3  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "4  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "5  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "6  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "7  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "8  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "9  <class 'set'>  <class 'str'>  <class 'tuple'>  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.applymap(lambda x: type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we didn't validate the dataset with a `type_map` the types were stored on ingestion and cast back on query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Failure\n",
    "\n",
    "There are a couple things that could make the upload fail, the primary two however are: \n",
    "\n",
    "1) if a value type doesn't match the specified value type in the `type_map` parameter\n",
    "\n",
    "2) if a value could not be cast to the specified value type when `import_as_type_map` parameter is `True`\n",
    "\n",
    "3) if a value does not return `True` from any validation function passed in the `validation_map` parameter\n",
    "\n",
    "4) if a filepath from one of the columns specified in the `filepath_columns` parameter does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "\n\nAllowed types: <class 'float'>\nGiven type: <class 'bool'>\nGiven value: True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-998c49315485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m ds_info = ingest.upload_dataset(database=db,\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                 type_map={\"bools\": float})\n\u001b[0m",
      "\u001b[0;32m/active/modelingdbtools/ingest.py\u001b[0m in \u001b[0;36mupload_dataset\u001b[0;34m(database, dataset, name, description, type_map, validation_map, upload_files, filepath_columns, import_as_type_map, use_unix_paths)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_unix_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mchecks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mchecks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menforce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/active/modelingdbtools/utils/checks.py\u001b[0m in \u001b[0;36mvalidate_dataset\u001b[0;34m(dataset, type_map, filepath_columns)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                         \u001b[0mcheck_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfilepath_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/active/modelingdbtools/utils/checks.py\u001b[0m in \u001b[0;36mcheck_types\u001b[0;34m(var, types, err)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCHECK_STRING_ERR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: \n\nAllowed types: <class 'float'>\nGiven type: <class 'bool'>\nGiven value: True\n"
     ]
    }
   ],
   "source": [
    "# cannot validate when a values type doesn't match the provided type_map\n",
    "ds_info = ingest.upload_dataset(database=db,\n",
    "                                dataset=test,\n",
    "                                type_map={\"bools\": float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Could not cast:', '/active/fp_example/0.json', 'to:', <class 'int'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/active/modelingdbtools/utils/handles.py\u001b[0m in \u001b[0;36mquick_cast\u001b[0;34m(value, cast_type)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcast_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '/active/fp_example/0.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fdbfc390f225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mtype_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"files\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                 import_as_type_map=True)\n\u001b[0m",
      "\u001b[0;32m/active/modelingdbtools/ingest.py\u001b[0m in \u001b[0;36mupload_dataset\u001b[0;34m(database, dataset, name, description, type_map, validation_map, upload_files, filepath_columns, import_as_type_map, use_unix_paths)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# validate dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimport_as_type_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_unix_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/active/modelingdbtools/utils/handles.py\u001b[0m in \u001b[0;36mformat_data\u001b[0;34m(dataset, type_map)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquick_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3192\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/active/modelingdbtools/utils/handles.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquick_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/active/modelingdbtools/utils/handles.py\u001b[0m in \u001b[0;36mquick_cast\u001b[0;34m(value, cast_type)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcast_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not cast:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Could not cast:', '/active/fp_example/0.json', 'to:', <class 'int'>)"
     ]
    }
   ],
   "source": [
    "# cannot validate when trying to cast non-integer strings to integers\n",
    "ds_info = ingest.upload_dataset(database=db,\n",
    "                                dataset=test,\n",
    "                                type_map={\"files\": int},\n",
    "                                import_as_type_map=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dataset failed data check at:\n\tcol:bools\n\trow:0\n\tvalue:True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7a1fac0ffc11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m ds_info = ingest.upload_dataset(database=db,\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                 validation_map={\"bools\": lambda x: isinstance(x, str)})\n\u001b[0m",
      "\u001b[0;32m/active/modelingdbtools/ingest.py\u001b[0m in \u001b[0;36mupload_dataset\u001b[0;34m(database, dataset, name, description, type_map, validation_map, upload_files, filepath_columns, import_as_type_map, use_unix_paths)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_unix_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mchecks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mchecks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menforce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# actual dataset name check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/active/modelingdbtools/utils/checks.py\u001b[0m in \u001b[0;36menforce_values\u001b[0;34m(dataset, validation_map)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                     \u001b[0;32massert\u001b[0m \u001b[0mvalidation_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Dataset failed data check at:\n\tcol:bools\n\trow:0\n\tvalue:True"
     ]
    }
   ],
   "source": [
    "# cannot validate a boolean as a string\n",
    "ds_info = ingest.upload_dataset(database=db,\n",
    "                                dataset=test,\n",
    "                                validation_map={\"bools\": lambda x: isinstance(x, str)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "\n\nThe provided filepath does not exist.\nGiven filepath: /active/fp_example/0.json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3cdaf47ba469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m ds_info = ingest.upload_dataset(database=db,\n\u001b[1;32m      5\u001b[0m                                 \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                 filepath_columns=[\"files\"])\n\u001b[0m",
      "\u001b[0;32m/active/modelingdbtools/ingest.py\u001b[0m in \u001b[0;36mupload_dataset\u001b[0;34m(database, dataset, name, description, type_map, validation_map, upload_files, filepath_columns, import_as_type_map, use_unix_paths)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_unix_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mchecks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mchecks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menforce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/active/modelingdbtools/utils/checks.py\u001b[0m in \u001b[0;36mvalidate_dataset\u001b[0;34m(dataset, type_map, filepath_columns)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfilepath_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                         \u001b[0mcheck_file_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0menforce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/active/modelingdbtools/utils/checks.py\u001b[0m in \u001b[0;36mcheck_file_exists\u001b[0;34m(f, err)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: \n\nThe provided filepath does not exist.\nGiven filepath: /active/fp_example/0.json\n"
     ]
    }
   ],
   "source": [
    "# cannot validate when a file doesn't exist\n",
    "shutil.rmtree(fp_ex)\n",
    "\n",
    "ds_info = ingest.upload_dataset(database=db,\n",
    "                                dataset=test,\n",
    "                                filepath_columns=[\"files\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
