{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Database Ingestion and Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "For this example, I will be creating a local modeling database that will live at `/database/local_store.db`. If you look at the `bash.sh` and `jupyter.sh` scripts for this project you will see how I am mounting those drives so that I can reuse this local database both on my base OS and in the basic docker image for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "# run these uploads locally\n",
    "serv = \"local\"\n",
    "configs = pathlib.Path(\"/database/configs.json\")\n",
    "with open(configs) as read_in:\n",
    "    configs = json.load(read_in)\n",
    "\n",
    "# setting local store path\n",
    "configs[serv][serv][\"database\"] = str(pathlib.Path(\"/database/local_store.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table: SourceType\n",
      "Created table: User\n",
      "Created table: Iota\n",
      "Created table: Dataset\n",
      "Created table: IotaDatasetJunction\n",
      "Created table: Run\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from orator import DatabaseManager\n",
    "from modelingdbtools import query\n",
    "from modelingdbtools import ingest\n",
    "from modelingdbtools.utils import admin\n",
    "from modelingdbtools.schemas import modeling\n",
    "\n",
    "# create database connection\n",
    "db = DatabaseManager(configs[serv])\n",
    "# create and fill tables with basic data\n",
    "modeling.create_schema(db)\n",
    "modeling.add_schema_data(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Dataset\n",
    "\n",
    "The next cell will create an ingestion test dataset and a csv copy of the dataset saved to `/active/fp_example/example.csv` to show the capabilities of how data can be pushed to each instance of a modeling database using `modelingdbtools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bools</th>\n",
       "      <th>files</th>\n",
       "      <th>floats</th>\n",
       "      <th>ndarrays</th>\n",
       "      <th>sets</th>\n",
       "      <th>strings</th>\n",
       "      <th>tuples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/0.json</td>\n",
       "      <td>43.747389</td>\n",
       "      <td>[0.5643305755878399, 0.6632140606844696]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo0</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/1.json</td>\n",
       "      <td>81.263884</td>\n",
       "      <td>[0.7750176716004773, 0.7465286293692389]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo1</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/2.json</td>\n",
       "      <td>14.450579</td>\n",
       "      <td>[0.07404816355589705, 0.19810288785157115]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo2</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/3.json</td>\n",
       "      <td>2.169538</td>\n",
       "      <td>[0.4869247194197047, 0.3565979644712097]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo3</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/4.json</td>\n",
       "      <td>61.400925</td>\n",
       "      <td>[0.8442405148718909, 0.460814158230027]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo4</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/5.json</td>\n",
       "      <td>81.686670</td>\n",
       "      <td>[0.4561273303621597, 0.5840947991105992]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo5</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/6.json</td>\n",
       "      <td>60.853102</td>\n",
       "      <td>[0.5909996382449292, 0.19973470221660228]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo6</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/7.json</td>\n",
       "      <td>86.281845</td>\n",
       "      <td>[0.22382903044628077, 0.580813218405511]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo7</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/8.json</td>\n",
       "      <td>28.590351</td>\n",
       "      <td>[0.7780179492759283, 0.5806633350387581]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo8</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/9.json</td>\n",
       "      <td>47.813695</td>\n",
       "      <td>[0.3672095127812093, 0.7167949078477823]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo9</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bools                      files     floats  \\\n",
       "0   True  /active/fp_example/0.json  43.747389   \n",
       "1  False  /active/fp_example/1.json  81.263884   \n",
       "2  False  /active/fp_example/2.json  14.450579   \n",
       "3   True  /active/fp_example/3.json   2.169538   \n",
       "4  False  /active/fp_example/4.json  61.400925   \n",
       "5  False  /active/fp_example/5.json  81.686670   \n",
       "6   True  /active/fp_example/6.json  60.853102   \n",
       "7   True  /active/fp_example/7.json  86.281845   \n",
       "8   True  /active/fp_example/8.json  28.590351   \n",
       "9  False  /active/fp_example/9.json  47.813695   \n",
       "\n",
       "                                     ndarrays       sets strings     tuples  \n",
       "0    [0.5643305755878399, 0.6632140606844696]  {1, 2, 3}    foo0  (1, 2, 3)  \n",
       "1    [0.7750176716004773, 0.7465286293692389]  {1, 2, 3}    foo1  (1, 2, 3)  \n",
       "2  [0.07404816355589705, 0.19810288785157115]  {1, 2, 3}    foo2  (1, 2, 3)  \n",
       "3    [0.4869247194197047, 0.3565979644712097]  {1, 2, 3}    foo3  (1, 2, 3)  \n",
       "4     [0.8442405148718909, 0.460814158230027]  {1, 2, 3}    foo4  (1, 2, 3)  \n",
       "5    [0.4561273303621597, 0.5840947991105992]  {1, 2, 3}    foo5  (1, 2, 3)  \n",
       "6   [0.5909996382449292, 0.19973470221660228]  {1, 2, 3}    foo6  (1, 2, 3)  \n",
       "7    [0.22382903044628077, 0.580813218405511]  {1, 2, 3}    foo7  (1, 2, 3)  \n",
       "8    [0.7780179492759283, 0.5806633350387581]  {1, 2, 3}    foo8  (1, 2, 3)  \n",
       "9    [0.3672095127812093, 0.7167949078477823]  {1, 2, 3}    foo9  (1, 2, 3)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create test upload dataset\n",
    "fp_ex = pathlib.Path(\"/active/fp_example/\")\n",
    "if not os.path.exists(fp_ex):\n",
    "    os.makedirs(fp_ex)\n",
    "\n",
    "test = []\n",
    "for i in range(10):\n",
    "    fp =  fp_ex / (str(i) + \".json\")\n",
    "    with open(fp, \"w\") as write_out:\n",
    "        json.dump({\"hello\": \"world\"}, write_out)\n",
    "    \n",
    "    d = {}\n",
    "    d[\"strings\"] = \"foo\" + str(i)\n",
    "    d[\"bools\"] = np.random.rand() < 0.5\n",
    "    d[\"floats\"] = np.random.rand() * 100\n",
    "    d[\"ndarrays\"] = np.random.rand(2)\n",
    "    d[\"tuples\"] = tuple([1, 2, 3])\n",
    "    d[\"sets\"] = set([1, 2, 3, 3, 3])\n",
    "    d[\"files\"] = str(fp)\n",
    "    test.append(d)\n",
    "\n",
    "test = pd.DataFrame(test)\n",
    "test.to_csv(fp_ex / \"example.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the Dataset\n",
    "\n",
    "What we just did was make a dataset with various types of values to show that the database will handle all classes of data that can be stringified. There are various reasons for doing this, mainly execution time to query, merge, and join datasets when we only want certain columns from each row being the largest. This could have been done in NoSQL but would have required much more time to develop and as a starting proof-of-concept this worked well to get it into peoples hands and testing the capabilities. Development on a NoSQL version may come later but for now SQL is suiting us fine.\n",
    "\n",
    "With various types created however let's upload and see how the database handles the data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DatasetId</th>\n",
       "      <th>Description</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>this is the hello world of dataset ingestion</td>\n",
       "      <td>test_dataset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DatasetId                                   Description          Name\n",
       "0          1  this is the hello world of dataset ingestion  test_dataset"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload the test dataset to the connected db\n",
    "# map each column to a python class/ object\n",
    "# indicate not to upload the files to the fms\n",
    "# indicate which files should be checked for existance\n",
    "ds_info = ingest.upload_dataset(database=db,\n",
    "                                dataset=test,\n",
    "                                name=\"test_dataset\",\n",
    "                                description=\"this is the hello world of dataset ingestion\",\n",
    "                                type_map={\"bools\": bool, \n",
    "                                          \"files\": str, \n",
    "                                          \"floats\": float, \n",
    "                                          \"ndarrays\": np.ndarray, \n",
    "                                          \"strings\": str},\n",
    "                                upload_files=False,\n",
    "                                filepath_columns=[\"files\"])\n",
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any of this information is stored, the entire dataset is validated for two things, does all the data match the typing provided by the `type_map` parameters and do all the files exist in the columns provided by the `filepath_columns` parameter. Once the dataset is validated, what is actually happening on the backend is that the dataframe is being broken into individual key value pairings each with a source, sourcetype, group (row), valuetype, and creation datetime. This allows you to reduce down datasets to only keys (columns) you care about incredibly quickly, and return back to the original data typing when ingestion occured.\n",
    "\n",
    "If you want to validate a dataframe but do not want to upload the dataset to the database you can use:\n",
    "\n",
    "```\n",
    "from modelingdbtools.utils import checks\n",
    "checks.validate_dataset(dataset, type_map, filepath_columns)\n",
    "```\n",
    "\n",
    "I built a casting function that casts the string values stored back to their original types (in my opinion it covers pretty much all the types that you should really stick into the database, anything else and I think you may be trying to shove too much into the database... ([details on value casting here](./testing_decode_types.ipynb))\n",
    "\n",
    "To get the dataset pass either `ds_info[\"DatasetId\"]` or `ds_info[\"Name\"]` into `modelingdbtools.query.get_dataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bools</th>\n",
       "      <th>files</th>\n",
       "      <th>floats</th>\n",
       "      <th>ndarrays</th>\n",
       "      <th>sets</th>\n",
       "      <th>strings</th>\n",
       "      <th>tuples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/0.json</td>\n",
       "      <td>43.747389</td>\n",
       "      <td>[0.56433058, 0.66321406]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo0</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/1.json</td>\n",
       "      <td>81.263884</td>\n",
       "      <td>[0.77501767, 0.74652863]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo1</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/2.json</td>\n",
       "      <td>14.450579</td>\n",
       "      <td>[0.07404816, 0.19810289]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo2</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/3.json</td>\n",
       "      <td>2.169538</td>\n",
       "      <td>[0.48692472, 0.35659796]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo3</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/4.json</td>\n",
       "      <td>61.400925</td>\n",
       "      <td>[0.84424051, 0.46081416]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo4</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/5.json</td>\n",
       "      <td>81.686670</td>\n",
       "      <td>[0.45612733, 0.5840948]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo5</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/6.json</td>\n",
       "      <td>60.853102</td>\n",
       "      <td>[0.59099964, 0.1997347]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo6</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/7.json</td>\n",
       "      <td>86.281845</td>\n",
       "      <td>[0.22382903, 0.58081322]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo7</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>/active/fp_example/8.json</td>\n",
       "      <td>28.590351</td>\n",
       "      <td>[0.77801795, 0.58066334]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo8</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>/active/fp_example/9.json</td>\n",
       "      <td>47.813695</td>\n",
       "      <td>[0.36720951, 0.71679491]</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>foo9</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bools                      files     floats                  ndarrays  \\\n",
       "0   True  /active/fp_example/0.json  43.747389  [0.56433058, 0.66321406]   \n",
       "1  False  /active/fp_example/1.json  81.263884  [0.77501767, 0.74652863]   \n",
       "2  False  /active/fp_example/2.json  14.450579  [0.07404816, 0.19810289]   \n",
       "3   True  /active/fp_example/3.json   2.169538  [0.48692472, 0.35659796]   \n",
       "4  False  /active/fp_example/4.json  61.400925  [0.84424051, 0.46081416]   \n",
       "5  False  /active/fp_example/5.json  81.686670   [0.45612733, 0.5840948]   \n",
       "6   True  /active/fp_example/6.json  60.853102   [0.59099964, 0.1997347]   \n",
       "7   True  /active/fp_example/7.json  86.281845  [0.22382903, 0.58081322]   \n",
       "8   True  /active/fp_example/8.json  28.590351  [0.77801795, 0.58066334]   \n",
       "9  False  /active/fp_example/9.json  47.813695  [0.36720951, 0.71679491]   \n",
       "\n",
       "        sets strings     tuples  \n",
       "0  {1, 2, 3}    foo0  (1, 2, 3)  \n",
       "1  {1, 2, 3}    foo1  (1, 2, 3)  \n",
       "2  {1, 2, 3}    foo2  (1, 2, 3)  \n",
       "3  {1, 2, 3}    foo3  (1, 2, 3)  \n",
       "4  {1, 2, 3}    foo4  (1, 2, 3)  \n",
       "5  {1, 2, 3}    foo5  (1, 2, 3)  \n",
       "6  {1, 2, 3}    foo6  (1, 2, 3)  \n",
       "7  {1, 2, 3}    foo7  (1, 2, 3)  \n",
       "8  {1, 2, 3}    foo8  (1, 2, 3)  \n",
       "9  {1, 2, 3}    foo9  (1, 2, 3)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the dataset we just uploaded\n",
    "query.get_dataset(db, id=ds_info[\"DatasetId\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This same ingestion process can handle a csv filepath as well however there are some limitations in that when `pandasDataFrame`s are sent to a csv they lose complex typing such as `numpy.ndarray`. In this case we will specify that the `ndarrays` column should be validated as `string`.\n",
    "\n",
    "Validation of `string` can be done by explictly stating so, or by leaving the that key in the `type_map` blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dataset with that name already exists. Adding new version.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DatasetId</th>\n",
       "      <th>Description</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>this is the hello world of dataset ingestion</td>\n",
       "      <td>test_dataset@@2018-06-14 00:26:44.819390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DatasetId                                   Description  \\\n",
       "0          2  this is the hello world of dataset ingestion   \n",
       "\n",
       "                                       Name  \n",
       "0  test_dataset@@2018-06-14 00:26:44.819390  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also upload this with a csv filepath\n",
    "# you may lose typing on complex types though (ndarrays -> str)\n",
    "ds_info = ingest.upload_dataset(database=db,\n",
    "                                name=\"test_dataset\",\n",
    "                                dataset=(fp_ex / \"example.csv\"),\n",
    "                                description=\"this is the hello world of dataset ingestion\",\n",
    "                                type_map={\"bools\": bool, \n",
    "                                          \"files\": str, \n",
    "                                          \"floats\": float, \n",
    "                                          \"strings\": str},\n",
    "                                upload_files=False,\n",
    "                                filepath_columns=[\"files\"])\n",
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try filtering down the dataset we just uploaded to only return the `ndarray`s but before we do so, note that because we said to use the same dataset name as before, this upload was given a version (datetime) attached to the name. Additionally, if you want to get all the info regarding each `Iota` then use the `get_info_items` parameter.\n",
    "\n",
    "Note: `columns` parameter can be passed a `string` or `list of strings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndarrays</th>\n",
       "      <th>ndarrays(IotaId)</th>\n",
       "      <th>ndarrays(SourceId)</th>\n",
       "      <th>ndarrays(SourceTypeId)</th>\n",
       "      <th>ndarrays(Type)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.56433058 0.66321406]</td>\n",
       "      <td>75</td>\n",
       "      <td>test_dataset@@2018-06-14 00:26:44.819390</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.77501767 0.74652863]</td>\n",
       "      <td>83</td>\n",
       "      <td>test_dataset@@2018-06-14 00:26:44.819390</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.07404816 0.19810289]</td>\n",
       "      <td>91</td>\n",
       "      <td>test_dataset@@2018-06-14 00:26:44.819390</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.48692472 0.35659796]</td>\n",
       "      <td>99</td>\n",
       "      <td>test_dataset@@2018-06-14 00:26:44.819390</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.84424051 0.46081416]</td>\n",
       "      <td>107</td>\n",
       "      <td>test_dataset@@2018-06-14 00:26:44.819390</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.45612733 0.5840948 ]</td>\n",
       "      <td>115</td>\n",
       "      <td>test_dataset@@2018-06-14 00:26:44.819390</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.59099964 0.1997347 ]</td>\n",
       "      <td>123</td>\n",
       "      <td>test_dataset@@2018-06-14 00:26:44.819390</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.22382903 0.58081322]</td>\n",
       "      <td>131</td>\n",
       "      <td>test_dataset@@2018-06-14 00:26:44.819390</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.77801795 0.58066334]</td>\n",
       "      <td>139</td>\n",
       "      <td>test_dataset@@2018-06-14 00:26:44.819390</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.36720951 0.71679491]</td>\n",
       "      <td>147</td>\n",
       "      <td>test_dataset@@2018-06-14 00:26:44.819390</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ndarrays  ndarrays(IotaId)  \\\n",
       "0  [0.56433058 0.66321406]                75   \n",
       "1  [0.77501767 0.74652863]                83   \n",
       "2  [0.07404816 0.19810289]                91   \n",
       "3  [0.48692472 0.35659796]                99   \n",
       "4  [0.84424051 0.46081416]               107   \n",
       "5  [0.45612733 0.5840948 ]               115   \n",
       "6  [0.59099964 0.1997347 ]               123   \n",
       "7  [0.22382903 0.58081322]               131   \n",
       "8  [0.77801795 0.58066334]               139   \n",
       "9  [0.36720951 0.71679491]               147   \n",
       "\n",
       "                         ndarrays(SourceId)  ndarrays(SourceTypeId)  \\\n",
       "0  test_dataset@@2018-06-14 00:26:44.819390                       1   \n",
       "1  test_dataset@@2018-06-14 00:26:44.819390                       1   \n",
       "2  test_dataset@@2018-06-14 00:26:44.819390                       1   \n",
       "3  test_dataset@@2018-06-14 00:26:44.819390                       1   \n",
       "4  test_dataset@@2018-06-14 00:26:44.819390                       1   \n",
       "5  test_dataset@@2018-06-14 00:26:44.819390                       1   \n",
       "6  test_dataset@@2018-06-14 00:26:44.819390                       1   \n",
       "7  test_dataset@@2018-06-14 00:26:44.819390                       1   \n",
       "8  test_dataset@@2018-06-14 00:26:44.819390                       1   \n",
       "9  test_dataset@@2018-06-14 00:26:44.819390                       1   \n",
       "\n",
       "  ndarrays(Type)  \n",
       "0  <class 'str'>  \n",
       "1  <class 'str'>  \n",
       "2  <class 'str'>  \n",
       "3  <class 'str'>  \n",
       "4  <class 'str'>  \n",
       "5  <class 'str'>  \n",
       "6  <class 'str'>  \n",
       "7  <class 'str'>  \n",
       "8  <class 'str'>  \n",
       "9  <class 'str'>  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get ndarrays columns from the dataset we just uploaded\n",
    "query.get_dataset(db, id=ds_info[\"DatasetId\"][0], columns=\"ndarrays\", get_info_items=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Upload Requirements\n",
    "\n",
    "Below is an example of the minimum parameters needed to successfully upload a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DatasetId</th>\n",
       "      <th>Description</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>maxfield@@2018-06-14 00:26:45.493605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DatasetId Description                                  Name\n",
       "0          3        None  maxfield@@2018-06-14 00:26:45.493605"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minimum you need to pass to upload a dataset\n",
    "ds_info = ingest.upload_dataset(database=db, dataset=test)\n",
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset name genereated was the `{user}@@{upload_datetime}`. If you are doing a minimal upload with a filepath instead of a `pandas.DataFrame` the name is generated using `{absolute_filepath}@@{upload_datetime}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset we just uploaded\n",
    "out = query.get_dataset(db, id=ds_info[\"DatasetId\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bools</th>\n",
       "      <th>files</th>\n",
       "      <th>floats</th>\n",
       "      <th>ndarrays</th>\n",
       "      <th>sets</th>\n",
       "      <th>strings</th>\n",
       "      <th>tuples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'float'&gt;</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>&lt;class 'tuple'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            bools          files           floats                 ndarrays  \\\n",
       "0  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "1  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "2  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "3  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "4  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "5  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "6  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "7  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "8  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "9  <class 'bool'>  <class 'str'>  <class 'float'>  <class 'numpy.ndarray'>   \n",
       "\n",
       "            sets        strings           tuples  \n",
       "0  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "1  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "2  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "3  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "4  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "5  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "6  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "7  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "8  <class 'set'>  <class 'str'>  <class 'tuple'>  \n",
       "9  <class 'set'>  <class 'str'>  <class 'tuple'>  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.applymap(lambda x: type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we didn't validate the dataset with a `type_map` the types were stored on ingestion and cast back on query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Failure\n",
    "\n",
    "There are a couple things that could make the upload fail, the primary two however are: \n",
    "\n",
    "1) if a value type doesn't match the specified value type in the `type_map` parameter\n",
    "\n",
    "2) if a filepath from one of the columns specified in the `filepath_columns` parameter does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "\n\nAllowed types: <class 'float'>\nGiven type: <class 'bool'>\nGiven value: True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8111c58f265e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                           \"strings\": type(None)},\n\u001b[1;32m     13\u001b[0m                                 \u001b[0mupload_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                 filepath_columns=[\"files\"])\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mds_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/modelingdbtools/ingest.py\u001b[0m in \u001b[0;36mupload_dataset\u001b[0;34m(database, dataset, name, description, type_map, upload_files, filepath_columns)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# validate dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mchecks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# actual dataset name check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/modelingdbtools/utils/checks.py\u001b[0m in \u001b[0;36mvalidate_dataset\u001b[0;34m(dataset, type_map, filepath_columns)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                     \u001b[0mcheck_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilepath_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/modelingdbtools/utils/checks.py\u001b[0m in \u001b[0;36mcheck_types\u001b[0;34m(var, types, err)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCHECK_STRING_ERR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: \n\nAllowed types: <class 'float'>\nGiven type: <class 'bool'>\nGiven value: True\n"
     ]
    }
   ],
   "source": [
    "# upload failure\n",
    "# a dataset will not be uploaded if any data or filepath validation fails\n",
    "ds_info = ingest.upload_dataset(database=db,\n",
    "                                dataset=test,\n",
    "                                name=\"test_dataset\",\n",
    "                                description=\"this is the hello world of dataset ingestion\",\n",
    "                                # let's change the types in type_map\n",
    "                                type_map={\"bools\": float, \n",
    "                                          \"files\": int, \n",
    "                                          \"floats\": bool, \n",
    "                                          \"ndarrays\": pd.DataFrame, \n",
    "                                          \"strings\": type(None)},\n",
    "                                upload_files=False,\n",
    "                                filepath_columns=[\"files\"])\n",
    "ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "\n\nThe provided filepath does not exist.\nGiven filepath: /active/fp_example/0.json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-64b5e9fe1787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                           \"strings\": str},\n\u001b[1;32m     13\u001b[0m                                 \u001b[0mupload_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                 filepath_columns=[\"files\"])\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mds_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/modelingdbtools/ingest.py\u001b[0m in \u001b[0;36mupload_dataset\u001b[0;34m(database, dataset, name, description, type_map, upload_files, filepath_columns)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# validate dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mchecks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# actual dataset name check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/modelingdbtools/utils/checks.py\u001b[0m in \u001b[0;36mvalidate_dataset\u001b[0;34m(dataset, type_map, filepath_columns)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilepath_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                     \u001b[0mcheck_file_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/modelingdbtools/utils/checks.py\u001b[0m in \u001b[0;36mcheck_file_exists\u001b[0;34m(f, err)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: \n\nThe provided filepath does not exist.\nGiven filepath: /active/fp_example/0.json\n"
     ]
    }
   ],
   "source": [
    "# let's remove the files and try to upload the dataset\n",
    "shutil.rmtree(fp_ex)\n",
    "\n",
    "ds_info = ingest.upload_dataset(database=db,\n",
    "                                dataset=test,\n",
    "                                name=\"test_dataset\",\n",
    "                                description=\"this is the hello world of dataset ingestion\",\n",
    "                                type_map={\"bools\": bool, \n",
    "                                          \"files\": str, \n",
    "                                          \"floats\": float, \n",
    "                                          \"ndarrays\": np.ndarray, \n",
    "                                          \"strings\": str},\n",
    "                                upload_files=False,\n",
    "                                filepath_columns=[\"files\"])\n",
    "ds_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
